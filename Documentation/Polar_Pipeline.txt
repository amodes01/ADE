1. News_Corpus_Collector:

Dependencies: keybert, newspaper3k

Parameters: Output_Dir, Start_Date, End_Date, keywords

Input: GDELT API

Output: dumps, html, articles, pre_processed

 

2. Entity_Extractor:

Dependencies:  spacy nlp, dbpedia, pandas 

Parameters: Output_Dir, n_processes

Input: pre_processed articles

Output: entities

 

3. Noun_Phrase Extractor:

Dependencies:  nltk, spacy

Parameters: Output_Dir, spacy_model_str, n_processes

Input: entities, articles

Output: noun_phrases folder

 

4. Topic_Identifier:

Dependencies:  torch, numpy, nltk, transformers, Nvidia CPU

Parameters: Output_Dir, chunk_size (5000), threshold(0.80)

Input: noun_phrases

Output: topics.json.gz file

 

5. Sentiment_Attitude_Pipeline:

Dependencies:  transformers, mpqa lexicon, spacy, textBlob, networkx, cuda

Parameters: Output_Dir, nlp model, mpqa path

Input: noun_phrases

Output: attitudes folder

 

6. SAGGenerator:

Dependencies:  numpy, matplotlib, itertools, networkx

Parameters: Output_Dir, figsize, bin_category_mapping, minimum_frequency, verbose

Input: attitudes folder

Output: polarization folder

 

7. Fellowship_Extractor:

Dependencies:  numpy, pandas, gzip, scipy, pickle, java, gurobi

Parameters: Output_Dir, n_iter, resolution, merg_iter, jar_path, verbose

Input: polarization folder

Output: polarization/ fellowships.json

 

8. Dipole_Generatior:

Dependencies:  numpy, pandas, gzip, scipy, pickle, java, gurobi

Parameters: Output_Dir, f_g_thr, n_r_thr

Input: polarization folder + fellowships.json

Output: polarization/dipoles.pckl

 

9. Topic_Attitude_Calculator:

Dependencies:  numpy, pandas, gzip, scipy, pickle, java, gurobi

Parameters: Output_Dir, entity_filter_list, merge_dict

Input: polarization folder + topics.json.gz + noun_phrases

 

Load_Sentiment_Attitudes:

get_polarization_topics:

Input: Dipoles

Returns: Dipole info + dictionary

 

get_topic_attitudes:

Parameters: aggr_func = numpy.mean

Output: Polarization/attitudes.pckl

 

 

Current GPT Code:

Replaces steps 1-5

Dependencies:  requests, Api Key

Parameters: Output_Dir, temperature, prompt, window_size

Input: pre_processed article parts

Output: json format entity relationships

 

 

Έβαλα επίσης κάποια επιπρόσθετα restrictions στο prompt για το GPT για να είναι πιο παρόμοια τα αποτελέσματα που επιστρέφει, προσπαθόντας κάθε φορά που αναφέρει ένα entity ή topic να το αναφέρει με τον ίδιο τρόπο, δεν κατάφερα να το δοκιμάσω λόγο κάποια request errors στο free GPT για να δώ αν άλλαξε το αποτέλεσμα. Έχω τώρα και code για να κάνω refactor το .json output του GPT στο format που χρειάζεται το SAGGenerator για να μπορέσω να το προσθέσω μέσα στο pipeline το GPT, αλλά χρειάζεται να λυθεί πρώτα το πρόβλημα με τα mentions στο ίδιο entity με διαφορετικούς τρόπους, πχ. (President Obama != Barack Obama), αλλοιώς θα χρειαστεί να κάνω άλλο code που θα τα ελέγχει όλα μετά με χρήση του dbpedia σαν post-processing.